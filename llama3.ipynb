{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = Ollama(model=\"llama3\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "result = chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love a good thought experiment!\n",
      "\n",
      "So, you're asking me what LangChain is? That sounds like an intriguing topic! \n",
      "\n",
      "As I take a step back and consider it... Hmm, could LangChain be related to the field of Natural Language Processing (NLP)? Maybe it's some sort of AI-powered language model that can generate human-like text or even converse with humans?\n",
      "\n",
      "Am I on the right track?\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Nodelling_' from 'src.modelling' (/home/preeti/Desktop/sumit/pdf2ppt/src/modelling.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Nodelling_\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Nodelling_' from 'src.modelling' (/home/preeti/Desktop/sumit/pdf2ppt/src/modelling.py)"
     ]
    }
   ],
   "source": [
    "from src.modelling import Nodelling_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias_detection_and_mitigation.py  llama3.ipynb         requrement.txt  \u001b[0m\u001b[01;34mtables\u001b[0m/\n",
      "\u001b[01;34mdataset\u001b[0m/                          PDF_TO_PPT_V2.ipynb  \u001b[01;34mresults\u001b[0m/\n",
      "\u001b[01;34mimages\u001b[0m/                           req.txt              \u001b[01;34msrc\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"\n",
    "**Slide 1: Title Slide**\n",
    "Title: BLEU Training Cost (FLOPs) for Machine Translation Models\n",
    "Subtitle: A Comparative Analysis of Different Architectures and Techniques\n",
    "\n",
    "[Image: A diagram of a multi-layered neural network]\n",
    "\n",
    "**Slide 2: Introduction**\n",
    "Points:\n",
    "* BLEU score is a widely used metric to evaluate the quality of machine translation models\n",
    "* Training cost, measured in FLOPs (floating-point operations per second), is an important consideration for deploying these models in production environments\n",
    "\n",
    "[Image: A chart showing the importance of training cost and BLEU score]\n",
    "\n",
    "**Slide 3: Different Architectures**\n",
    "Points:\n",
    "* ByteNet: a compact and efficient neural network architecture\n",
    "* Deep-Att + PosUnk: combines deep learning with attention mechanisms and unknown word handling\n",
    "* GNMT + RL: incorporates reinforcement learning for improved translation quality\n",
    "* ConvS2S: uses convolutional neural networks for sequence-to-sequence translation\n",
    "* MoE: employs mixture-of-experts models to improve translation accuracy\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression patterns\n",
    "slide_pattern = r'\\*\\*Slide (\\d+): (.+?)\\*\\*'\n",
    "points_pattern = r'Points:(.+?)(?=\\n\\n\\[|\\Z)'\n",
    "\n",
    "# Extracting slide titles and points\n",
    "slide_titles = re.findall(slide_pattern, text, re.DOTALL)\n",
    "slide_points = re.findall(points_pattern, text, re.DOTALL)\n",
    "\n",
    "title = []\n",
    "points = []\n",
    "\n",
    "title.append(slide_titles)\n",
    "points.append(slide_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('1', 'Title Slide'),\n",
       "  ('2', 'Introduction'),\n",
       "  ('3', 'Different Architectures')]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title\n",
    "# slide_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* BLEU score is a widely used metric to evaluate the quality of machine translation models\n",
      "* Training cost, measured in FLOPs (floating-point operations per second), is an important consideration for deploying these models in production environments\n"
     ]
    }
   ],
   "source": [
    "print(points[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_slides(text):\n",
    "    slide_pattern = re.compile(r'Slide (\\d+):\\nTitle: (.+?)\\nPoints:(.+?)(?=Slide \\d+:|$)', re.DOTALL)\n",
    "    point_pattern = re.compile(r'(.+?)\\n')#(r'(.+?)\\n')\n",
    "    matches = slide_pattern.findall(text)\n",
    "    \n",
    "\n",
    "    # Regular expression patterns\n",
    "    slide_pattern = r'\\*\\*Slide (\\d+): (.+?)\\*\\*'\n",
    "    points_pattern = r'Points:(.+?)(?=\\n\\n\\[|\\Z)'\n",
    "\n",
    "    # Extracting slide titles and points\n",
    "    slide_titles = re.findall(slide_pattern, text, re.DOTALL)\n",
    "    slide_points = re.findall(points_pattern, text, re.DOTALL)\n",
    "\n",
    "    final_list=[]\n",
    "    for match in slide_titles:\n",
    "        slide_number = match[0]\n",
    "        title = match[1].strip()\n",
    "        # points = point_pattern.findall(match[2])\n",
    "        dct={}\n",
    "        dct[\"Topic\"]= title\n",
    "        dct[\"Summary\"] = match[2]\n",
    "        final_list.append(dct)\n",
    "    return final_list\n",
    "\n",
    "slides = extract_slides(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_slides(text):\n",
    "    slides = []\n",
    "    slide_pattern = re.compile(r'Slide (\\d+):\\nTitle: (.+?)\\nPoints:(.+?)(?=Slide \\d+:|\\Z)', re.DOTALL)\n",
    "    point_pattern = re.compile(r'\\* (.+?)\\n')  # Pattern to extract each point\n",
    "\n",
    "    matches = slide_pattern.findall(text)\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for match in matches:\n",
    "        slide_number = match[0]\n",
    "        title = match[1].strip()\n",
    "        points_match = point_pattern.findall(match[2])\n",
    "        points = [point.strip() for point in points_match]\n",
    "        \n",
    "        slide_dict = {\n",
    "            \"Topic\": title,\n",
    "            \"Summary\": points\n",
    "        }\n",
    "        \n",
    "        final_list.append(slide_dict)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "Slide 1:\n",
    "Title: BLEU Training Cost (FLOPs) for Machine Translation Models\n",
    "Points:\n",
    "* BLEU score is a widely used metric to evaluate the quality of machine translation models\n",
    "* Training cost, measured in FLOPs (floating-point operations per second), is an important consideration for deploying these models in production environments\n",
    "\n",
    "Slide 2:\n",
    "Title: Introduction\n",
    "Points:\n",
    "* This is the first point of the introduction.\n",
    "* This is the second point of the introduction.\n",
    "\n",
    "Slide 3:\n",
    "Title: Different Architectures\n",
    "Points:\n",
    "* ByteNet: a compact and efficient neural network architecture\n",
    "* Deep-Att + PosUnk: combines deep learning with attention mechanisms and unknown word handling\n",
    "* GNMT + RL: incorporates reinforcement learning for improved translation quality\n",
    "\"\"\"\n",
    "\n",
    "text1 = \"\"\"**Slide 1: Title Slide**\n",
    "Title: \"Recurrent Neural Networks and their Challenges\"\n",
    "Points:\n",
    "* Recent advancements in sequence modeling\n",
    "* Importance of parallelization in training examples\n",
    "\n",
    "[Image: A diagram of a multi-layered neural network]\n",
    "\n",
    "**Slide 2: Background**\n",
    "Title: \"Recurrent Neural Networks (RNNs)\"\n",
    "Points:\n",
    "* RNNs are state-of-the-art approaches for sequence modeling and transduction problems\n",
    "* Types of RNNs: Long Short-Term Memory (LSTM), Gated Recurrent Neural Network (GRU)\n",
    "\n",
    "[Text:] Recent work has achieved significant improvements in computational efficiency through factorization tricks  and conditional computation , while also improving model performance in case of the latter.\n",
    "\n",
    "**Slide 3: Challenges**\n",
    "Title: \"Challenges in Training RNNs\"\n",
    "Points:\n",
    "* Sequential nature of RNNs makes parallelization within training examples difficult\n",
    "* Memory constraints limit batching across examples at longer sequence lengths\"\"\"\n",
    "\n",
    "slides = extract_slides(text1)\n",
    "\n",
    "# Print the extracted slides in the desired format\n",
    "for slide in slides:\n",
    "    print(f\"Topic: {slide['Topic']}\")\n",
    "    print(\"Summary:\")\n",
    "    for point in slide['Summary']:\n",
    "        print(f\"* {point}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m slide_points \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(points_pattern, text1, re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[1;32m      9\u001b[0m final_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mslide_titles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslide_points\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch \u001b[39m\u001b[38;5;124m'\u001b[39m,match,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoint\u001b[39m\u001b[38;5;124m'\u001b[39m, m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# d = {}\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# d['title'] = \u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Regular expression patterns\n",
    "slide_pattern = r'\\*\\*Slide (\\d+): (.+?)\\*\\*'\n",
    "points_pattern = r'Points:(.+?)(?=\\n\\n\\[|\\Z)'\n",
    "\n",
    "# Extracting slide titles and points\n",
    "slide_titles = re.findall(slide_pattern, text1, re.DOTALL)\n",
    "slide_points = re.findall(points_pattern, text1, re.DOTALL)\n",
    "\n",
    "final_list = []\n",
    "for match, m in enumerate(slide_titles, slide_points):\n",
    "    print('match ',match,' ', 'point', m)\n",
    "    # d = {}\n",
    "    # d['title'] = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(slide_titles), len(slide_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Topic': 'Title Slide',\n",
       "  'Summary': ['* BLEU score is a widely used metric to evaluate the quality of machine translation models',\n",
       "   'Training cost, measured in FLOPs (floating-point operations per second), is an important consideration for deploying these models in production environments']},\n",
       " {'Topic': 'Different Architectures',\n",
       "  'Summary': ['* ByteNet: a compact and efficient neural network architecture',\n",
       "   'Deep-Att + PosUnk: combines deep learning with attention mechanisms and unknown word handling',\n",
       "   'GNMT + RL: incorporates reinforcement learning for improved translation quality',\n",
       "   'ConvS2S: uses convolutional neural networks for sequence-to-sequence translation',\n",
       "   'MoE: employs mixture-of-experts models to improve translation accuracy']}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_slides(text):\n",
    "    slide_pattern = r'\\*\\*Slide (\\d+): (.+?)\\*\\*[\\s\\S]*?Points:(.+?)(?=\\n\\n\\[|\\Z)'\n",
    "    slides = re.findall(slide_pattern, text, re.DOTALL)\n",
    "\n",
    "    final_list = []\n",
    "    for slide in slides:\n",
    "        slide_number = slide[0].strip()\n",
    "        title = slide[1].strip()\n",
    "        points = slide[2].strip().split('\\n* ') if slide[2].strip() else []\n",
    "\n",
    "        slide_dict = {\n",
    "            \"Topic\": title,\n",
    "            \"Summary\": points\n",
    "        }\n",
    "\n",
    "        final_list.append(slide_dict)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "# Example text1\n",
    "text1 = \"\"\"\n",
    "**Slide 1: Title Slide**\n",
    "Title: BLEU Training Cost (FLOPs) for Machine Translation Models\n",
    "Subtitle: A Comparative Analysis of Different Architectures and Techniques\n",
    "\n",
    "[Image: A diagram of a multi-layered neural network]\n",
    "\n",
    "**Slide 2: Introduction**\n",
    "Points:\n",
    "* BLEU score is a widely used metric to evaluate the quality of machine translation models\n",
    "* Training cost, measured in FLOPs (floating-point operations per second), is an important consideration for deploying these models in production environments\n",
    "\n",
    "[Image: A chart showing the importance of training cost and BLEU score]\n",
    "\n",
    "**Slide 3: Different Architectures**\n",
    "Points:\n",
    "* ByteNet: a compact and efficient neural network architecture\n",
    "* Deep-Att + PosUnk: combines deep learning with attention mechanisms and unknown word handling\n",
    "* GNMT + RL: incorporates reinforcement learning for improved translation quality\n",
    "* ConvS2S: uses convolutional neural networks for sequence-to-sequence translation\n",
    "* MoE: employs mixture-of-experts models to improve translation accuracy\n",
    "\"\"\"\n",
    "\n",
    "slides = extract_slides(text1)\n",
    "\n",
    "\n",
    "slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Topic': 'Title Slide',\n",
       "  'Summary': '\\n* BLEU score is a widely used metric to evaluate the quality of machine translation models\\n* Training cost, measured in FLOPs (floating-point operations per second), is an important consideration for deploying these models in production environments'},\n",
       " {'Topic': 'Different Architectures',\n",
       "  'Summary': '\\n* ByteNet: a compact and efficient neural network architecture\\n* Deep-Att + PosUnk: combines deep learning with attention mechanisms and unknown word handling\\n* GNMT + RL: incorporates reinforcement learning for improved translation quality\\n* ConvS2S: uses convolutional neural networks for sequence-to-sequence translation\\n* MoE: employs mixture-of-experts models to improve translation accuracy\\n'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_slides(text):\n",
    "    slide_pattern = r'\\*\\*Slide (\\d+): (.+?)\\*\\*[\\s\\S]*?Points:(.+?)(?=\\n\\n\\[|\\Z)'\n",
    "    slides = re.findall(slide_pattern, text, re.DOTALL)\n",
    "\n",
    "    final_list = []\n",
    "    for slide in slides:\n",
    "        slide_number = slide[0].strip()\n",
    "        title = slide[1].strip()\n",
    "        \n",
    "        slide_dict = {\n",
    "            \"Topic\": title,\n",
    "            \"Summary\": slide[2]\n",
    "        }\n",
    "\n",
    "        final_list.append(slide_dict)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "# Example text1\n",
    "text1 = \"\"\"\n",
    "**Slide 1: Title Slide**\n",
    "Title: BLEU Training Cost (FLOPs) for Machine Translation Models\n",
    "Subtitle: A Comparative Analysis of Different Architectures and Techniques\n",
    "\n",
    "[Image: A diagram of a multi-layered neural network]\n",
    "\n",
    "**Slide 2: Introduction**\n",
    "Points:\n",
    "* BLEU score is a widely used metric to evaluate the quality of machine translation models\n",
    "* Training cost, measured in FLOPs (floating-point operations per second), is an important consideration for deploying these models in production environments\n",
    "\n",
    "[Image: A chart showing the importance of training cost and BLEU score]\n",
    "\n",
    "**Slide 3: Different Architectures**\n",
    "Points:\n",
    "* ByteNet: a compact and efficient neural network architecture\n",
    "* Deep-Att + PosUnk: combines deep learning with attention mechanisms and unknown word handling\n",
    "* GNMT + RL: incorporates reinforcement learning for improved translation quality\n",
    "* ConvS2S: uses convolutional neural networks for sequence-to-sequence translation\n",
    "* MoE: employs mixture-of-experts models to improve translation accuracy\n",
    "\"\"\"\n",
    "\n",
    "slides = extract_slides(text1)\n",
    "\n",
    "\n",
    "slides\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "p = os.walk('tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _walk at 0x7bb49dd90970>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title Slide', 'Introduction', 'Different Architectures']\n",
      "* BLEU score is a widely used metric to evaluate the quality of machine translation models\n",
      "* Training cost, measured in FLOPs (floating-point operations per second), is an important consideration for deploying these models in production environments\n",
      "\n",
      "[Image: A chart showing the importance of training cost and BLEU score]\n",
      "ByteNet: a compact and efficient neural network architecture\n",
      "Deep-Att + PosUnk: combines deep learning with attention mechanisms and unknown word handling\n",
      "GNMT + RL: incorporates reinforcement learning for improved translation quality\n",
      "ConvS2S: uses convolutional neural networks for sequence-to-sequence translation\n",
      "MoE: employs mixture-of-experts models to improve translation accuracy\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "**Slide 1: Title Slide**\n",
    "Title: BLEU Training Cost (FLOPs) for Machine Translation Models\n",
    "Subtitle: A Comparative Analysis of Different Architectures and Techniques\n",
    "\n",
    "[Image: A diagram of a multi-layered neural network]\n",
    "\n",
    "**Slide 2: Introduction**\n",
    "Points:\n",
    "* BLEU score is a widely used metric to evaluate the quality of machine translation models\n",
    "* Training cost, measured in FLOPs (floating-point operations per second), is an important consideration for deploying these models in production environments\n",
    "\n",
    "[Image: A chart showing the importance of training cost and BLEU score]\n",
    "\n",
    "**Slide 3: Different Architectures**\n",
    "Points:\n",
    "* ByteNet: a compact and efficient neural network architecture\n",
    "* Deep-Att + PosUnk: combines deep learning with attention mechanisms and unknown word handling\n",
    "* GNMT + RL: incorporates reinforcement learning for improved translation quality\n",
    "* ConvS2S: uses convolutional neural networks for sequence-to-sequence translation\n",
    "* MoE: employs mixture-of-experts models to improve translation accuracy\n",
    "\"\"\"\n",
    "def extract_slide_titles(text):\n",
    "    # Regex pattern to match the slide titles\n",
    "    pattern = r'\\*\\*Slide \\d+: (.*?)\\*\\*'\n",
    "    \n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "slide_titles = extract_slide_titles(text1)\n",
    "\n",
    "# Print the extracted slide titles\n",
    "print(slide_titles)\n",
    "def extract_slide_points(text):\n",
    "    # Regex pattern to match points under each slide\n",
    "    pattern = r'\\*\\*Slide \\d+:.*?Points:(.*?)\\*\\*|\\* (.*?)\\n'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "    # Flatten the matches into a single list of points\n",
    "    points = [point.strip() for match in matches for point in match if point.strip()]\n",
    "    \n",
    "    return points\n",
    "\n",
    "slide_points = extract_slide_points(text1)\n",
    "\n",
    "# Print the extracted slide points\n",
    "for point in slide_points:\n",
    "    print(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "**Slide 1: Title Slide**\n",
    "\n",
    "Title: Evaluation of Transformer Model Variations\n",
    "Subtitle: Measuring the Impact of Different Components on English-to-German Translation\n",
    "\n",
    "[Image: A simple diagram or logo]\n",
    "\n",
    "**Slide 2: Introduction**\n",
    "\n",
    "Title: Importance of Evaluating Transformer Model Variations\n",
    "Points:\n",
    "• To evaluate the importance of different components of the Transformer model\n",
    "• Measure performance changes on English-to-German translation task (newstest2013)\n",
    "• Compare results with previously published single models\n",
    "\n",
    "[Image: A simple diagram illustrating the concept]\n",
    "\n",
    "**Slide 3: Experiment Setup**\n",
    "\n",
    "Title: Model Variations and Evaluation Metrics\n",
    "Points:\n",
    "• Vary the number of attention heads and attention key and value dimensions\n",
    "• Keep amount of computation constant (Section 3.2.2)\n",
    "• Measure performance on English-to-German translation task (newstest2013)\n",
    "\n",
    "[Image: A diagram illustrating the experiment setup]\n",
    "\n",
    "**Slide 4: Results - Attention Heads**\n",
    "\n",
    "Title: Impact of Varying Number of Attention Heads\n",
    "Points:\n",
    "• Table 3 rows (A): [insert table]\n",
    "• More attention heads improve performance on English-to-German translation task\n",
    "\n",
    "[Image: A simple diagram or graph showing the results]\n",
    "\n",
    "**Slide 5: Results - Attention Key and Value Dimensions**\n",
    "\n",
    "Title: Impact of Varying Attention Key and Value Dimensions\n",
    "Points:\n",
    "• Table 3 rows (A): [insert table]\n",
    "• Larger attention key and value dimensions improve performance on English-to-German translation task\n",
    "\n",
    "[Image: A simple diagram or graph showing the results]\n",
    "\n",
    "**Slide 6: Comparison with Previous Models**\n",
    "\n",
    "Title: Performance Comparison with Previously Published Single Models\n",
    "Points:\n",
    "• Our big model achieves a BLEU score of 41.0 on WMT 2014 English-to-French translation task\n",
    "• Outperforms previously published single models at less than 1/4 the training cost\n",
    "\n",
    "[Image: A simple diagram or graph showing the comparison]\n",
    "\n",
    "**Slide 7: References**\n",
    "\n",
    "Title: Relevant Research Papers\n",
    "Points:\n",
    "• [arXiv preprint arXiv: 1608.05859, 2016](https://arxiv.org/abs/1608.05859)\n",
    "• Rico Sennrich, Barry Haddow, and Alexandra Birch (2016) [1]\n",
    "• Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jiirgen Schmidhuber (2015) [2]\n",
    "• Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio (2014) [3]\n",
    "\n",
    "[Image: A simple diagram or logo]\n",
    "\n",
    "**Slide 8: Conclusion**\n",
    "\n",
    "Title: Importance of Evaluating Transformer Model Variations\n",
    "Points:\n",
    "• Varying different components of the Transformer model can significantly impact performance on English-to-German translation task\n",
    "• Our results demonstrate the importance of evaluating these variations to improve model performance\n",
    "\n",
    "[Image: A simple diagram or logo]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Topic': 'Title Slide',\n",
       "  'Summary': '• To evaluate the importance of different components of the Transformer model\\n• Measure performance changes on English-to-German translation task (newstest2013)\\n• Compare results with previously published single models'},\n",
       " {'Topic': 'Experiment Setup',\n",
       "  'Summary': '• Vary the number of attention heads and attention key and value dimensions\\n• Keep amount of computation constant (Section 3.2.2)\\n• Measure performance on English-to-German translation task (newstest2013)'},\n",
       " {'Topic': 'Results - Attention Heads',\n",
       "  'Summary': '• Table 3 rows (A): [insert table]\\n• More attention heads improve performance on English-to-German translation task'},\n",
       " {'Topic': 'Results - Attention Key and Value Dimensions',\n",
       "  'Summary': '• Table 3 rows (A): [insert table]\\n• Larger attention key and value dimensions improve performance on English-to-German translation task'},\n",
       " {'Topic': 'Comparison with Previous Models',\n",
       "  'Summary': '• Our big model achieves a BLEU score of 41.0 on WMT 2014 English-to-French translation task\\n• Outperforms previously published single models at less than 1/4 the training cost'},\n",
       " {'Topic': 'References',\n",
       "  'Summary': '• [arXiv preprint arXiv: 1608.05859, 2016](https://arxiv.org/abs/1608.05859)\\n• Rico Sennrich, Barry Haddow, and Alexandra Birch (2016) [1]\\n• Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jiirgen Schmidhuber (2015) [2]\\n• Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio (2014) [3]'},\n",
       " {'Topic': 'Conclusion',\n",
       "  'Summary': '• Varying different components of the Transformer model can significantly impact performance on English-to-German translation task\\n• Our results demonstrate the importance of evaluating these variations to improve model performance'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_slides(text):\n",
    "    slide_pattern = r'\\*\\*Slide (\\d+): (.+?)\\*\\*[\\s\\S]*?Points:(.+?)(?=\\n\\n\\[|\\Z)'\n",
    "    slides = re.findall(slide_pattern, text, re.DOTALL)\n",
    "\n",
    "    final_list = []\n",
    "    for slide in slides:\n",
    "        slide_number = slide[0].strip()\n",
    "        title = slide[1].strip()\n",
    "        \n",
    "        slide_dict = {\n",
    "            \"Topic\": title,\n",
    "            \"Summary\": slide[2].strip()\n",
    "        }\n",
    "\n",
    "        final_list.append(slide_dict)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "# slides = extract_slides(text1)\n",
    "slides = extract_slides(text)\n",
    "\n",
    "\n",
    "slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• To evaluate the importance of different components of the Transformer model\n",
      "• Measure performance changes on English-to-German translation task (newstest2013)\n",
      "• Compare results with previously published single models\n"
     ]
    }
   ],
   "source": [
    "print(slides[0]['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model = 'llama3')\n",
    "# llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = \"\"\"\n",
    "    You are a power point presentation specialist. You are asked to create\n",
    "    the content for a presentation about {topic}.\n",
    "    You have been given the following information to create a presentation:\n",
    "    ---\n",
    "    {information}.\n",
    "    ---\n",
    "    Structure the information in a way that it can be put in a power point\n",
    "    presentation. Each slide should have a title and content, with the content\n",
    "    being a summary of the information provided. Each slide should have one or\n",
    "    more sentences that capture the key points of the information.\n",
    "    Return the structured information as a JSON as follows.\n",
    "    Your answer should only contain the JSON - no markdown formatting.\n",
    " \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "prompt_examples = \"\"\"\n",
    "  Example:\n",
    "  {\"slides\": [\n",
    "  {\"title\": \"Slide 1\", \"content\": \"Content for slide 1\"},\n",
    "  {\"title\": \"Slide 2\", \"content\": \"Content for slide 2\"},\n",
    "  {\"title\": \"Slide 3\", \"content\": \"Content for slide 3\"},\n",
    "  ]}\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = \"the benefits of exercise\"\n",
    "test_information = \"\"\"\n",
    "Exercise plays a crucial role in maintaining both physical and mental health.\n",
    "Engaging in regular physical activity can significantly reduce the risk of\n",
    "chronic diseases such as heart disease, diabetes, and obesity. It also enhances\n",
    "muscular strength, flexibility, and endurance. Beyond physical benefits, exercise\n",
    "contributes to improved mental health by reducing symptoms of depression and anxiety,\n",
    "boosting mood through the release of endorphins, and improving cognitive function.\n",
    "It fosters a sense of well-being and can be a great way to manage stress.\n",
    "Overall, incorporating exercise into one's daily routine is a key factor in\n",
    "achieving a healthier and more balanced lifestyle.\n",
    "\"\"\"\n",
    "\n",
    "content_prompt = (\n",
    "    prompt_template.format(topic=test_topic, information=test_information)\n",
    "    + prompt_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# llm = Ollama(model=\"llama3\")\n",
    "\n",
    "chain = LLMChain(\n",
    "            llm = Ollama(model=\"llama3\"),\n",
    "            prompt=PromptTemplate.from_template(prompt)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/preeti/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`run` supports only one positional argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m powerpoint_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mYou are a PowerPoint presentation specialist. You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll get a list of slides, each\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mslide containing a title and content. You need to create a PowerPoint presentation\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m presentation_code \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpowerpoint_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mslides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     21\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn(.*?)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn```\u001b[39m\u001b[38;5;124m\"\u001b[39m, presentation_code, re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[1;32m     22\u001b[0m python_code \u001b[38;5;241m=\u001b[39m match[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/langchain/chains/base.py:599\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 599\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    601\u001b[0m         _output_key\n\u001b[1;32m    602\u001b[0m     ]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n",
      "\u001b[0;31mValueError\u001b[0m: `run` supports only one positional argument."
     ]
    }
   ],
   "source": [
    "powerpoint_prompt = \"\"\"\n",
    "You are a PowerPoint presentation specialist. You'll get a list of slides, each\n",
    "slide containing a title and content. You need to create a PowerPoint presentation\n",
    "based on the provided slides.\n",
    "But there is a catch: Instead of creating the presentation, provide python code\n",
    "that generates the PowerPoint presentation based on the provided slides.\n",
    "Use the package python-pptx to create the PowerPoint presentation.\n",
    "The presentation should be visually appealing and professionally designed.\n",
    "\n",
    "If the slides content contains more than one information, make bullet points.\n",
    "Save the presentation as 'presentation.pptx'.\n",
    "\n",
    "Your answer should only contain the python code, no explanatory text.\n",
    "\n",
    "Slides:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "presentation_code = chain.run(powerpoint_prompt + str(slides), True).text\n",
    "\n",
    "match = re.findall(r\"python\\n(.*?)\\n```\", presentation_code, re.DOTALL)\n",
    "python_code = match[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [{'Topic': 'Title Slide', 'Summary': '1. Varying the number of attention heads and key-value dimensions while keeping computation constant.\\n\\t2. Investigating the impact of different model architectures on performance.\\n\\t3. Analyzing the effect of varying training data sizes and types.'}, {'Topic': 'Performance Evaluation', 'Summary': '1. Measuring performance on the development set (newstest2013).\\n\\t2. Comparing model performance to previous state-of-the-art models.\\n\\t3. Analyzing the impact of varying model parameters on translation quality.'}, {'Topic': 'Big Model Performance', 'Summary': '1. Big model achieves a BLEU score of 41.0 on WMT 2014 English-to-French translation task.\\n\\t2. Outperforms previous single models with reduced training cost.\\n\\t3. Demonstrates the effectiveness of the Transformer architecture.'}, {'Topic': 'Model Variations - Attention', 'Summary': '1. Investigating the impact of varying attention head numbers and dimensions.\\n\\t2. Analyzing the effect of different attention mechanisms on performance.\\n\\t3. Comparing results to previous studies on attention-based models.'}, {'Topic': 'Algorithm Diagram', 'Summary': '1. Illustrating the multi-layered structure of the Transformer.\\n\\t2. Highlighting the self-attention mechanism and feed-forward neural network (FFNN) layers.\\n\\t3. Showing the flow of information through the model.'}, {'Topic': 'Comparison to Previous Models', 'Summary': '1. Comparing performance of big model to previous state-of-the-art models.\\n\\t2. Analyzing the impact of different model architectures on translation quality.\\n\\t3. Highlighting the reduced training cost and improved results of the big model.'}, {'Topic': 'Conclusion', 'Summary': '1. Summarizing the key findings on model variations and performance evaluation.\\n\\t2. Highlighting the effectiveness of the Transformer architecture and big model.\\n\\t3. Discussing potential future directions for research and improvement.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Topic': 'Title Slide',\n",
       "  'Summary': '1. Varying the number of attention heads and key-value dimensions while keeping computation constant.\\n\\t2. Investigating the impact of different model architectures on performance.\\n\\t3. Analyzing the effect of varying training data sizes and types.'},\n",
       " {'Topic': 'Performance Evaluation',\n",
       "  'Summary': '1. Measuring performance on the development set (newstest2013).\\n\\t2. Comparing model performance to previous state-of-the-art models.\\n\\t3. Analyzing the impact of varying model parameters on translation quality.'},\n",
       " {'Topic': 'Big Model Performance',\n",
       "  'Summary': '1. Big model achieves a BLEU score of 41.0 on WMT 2014 English-to-French translation task.\\n\\t2. Outperforms previous single models with reduced training cost.\\n\\t3. Demonstrates the effectiveness of the Transformer architecture.'},\n",
       " {'Topic': 'Model Variations - Attention',\n",
       "  'Summary': '1. Investigating the impact of varying attention head numbers and dimensions.\\n\\t2. Analyzing the effect of different attention mechanisms on performance.\\n\\t3. Comparing results to previous studies on attention-based models.'},\n",
       " {'Topic': 'Algorithm Diagram',\n",
       "  'Summary': '1. Illustrating the multi-layered structure of the Transformer.\\n\\t2. Highlighting the self-attention mechanism and feed-forward neural network (FFNN) layers.\\n\\t3. Showing the flow of information through the model.'},\n",
       " {'Topic': 'Comparison to Previous Models',\n",
       "  'Summary': '1. Comparing performance of big model to previous state-of-the-art models.\\n\\t2. Analyzing the impact of different model architectures on translation quality.\\n\\t3. Highlighting the reduced training cost and improved results of the big model.'},\n",
       " {'Topic': 'Conclusion',\n",
       "  'Summary': '1. Summarizing the key findings on model variations and performance evaluation.\\n\\t2. Highlighting the effectiveness of the Transformer architecture and big model.\\n\\t3. Discussing potential future directions for research and improvement.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pdf_2_ppt import RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-31 10:10:21,087: INFO: pdf_2_ppt: 13: ======RAG Function Started======]\n",
      "[2024-07-31 10:10:21,090: INFO: pdf_2_ppt: 39: ======Extracting Text, Images and Tables from PDF file======]\n",
      "[2024-07-31 10:10:21,105: INFO: <frozen importlib: 241: pikepdf C++ to Python logger bridge initialized]\n",
      "[2024-07-31 10:10:23,410: INFO: layout: 59: Reading PDF for file: dataset/attention_all_you_need.pdf ...]\n",
      "[2024-07-31 10:10:32,697: INFO: tables: 138: Loading the Table agent ...]\n",
      "[2024-07-31 10:10:32,698: INFO: tables: 67: Loading the table structure model ...]\n",
      "[2024-07-31 10:10:33,595: INFO: _builder: 187: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)]\n",
      "[2024-07-31 10:10:33,827: INFO: _hub: 180: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.]\n",
      "[2024-07-31 10:10:33,847: INFO: _builder: 245: Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++ inside user prompt ++++++\n",
      "[2024-07-31 10:11:10,625: INFO: vectordb: 14: ======Vector-DB Function Started======]\n",
      "[2024-07-31 10:11:10,629: INFO: vectordb: 18: ======Generating Embeddings======]\n",
      "[2024-07-31 10:11:10,630: INFO: summary: 19: ======Summary Function Started======]\n",
      "[2024-07-31 10:11:10,630: INFO: summary: 29: ======Text Summary Started======]\n",
      "modelling constructor\n",
      "[2024-07-31 10:11:10,631: INFO: modelling: 14: ======Loading Model======]\n",
      "[2024-07-31 10:11:10,635: INFO: modelling: 20: ======Model Loaded Successfully======]\n",
      "[2024-07-31 10:12:18,050: INFO: helper: 10: ======Helper Functin Started======]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/preeti/Desktop/sumit/pdf2ppt/src/helper.py:24: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  sentences = [(sent.text.strip(), sent.similarity(doc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-31 10:12:19,452: INFO: helper: 38: ======Helper Functin Ended======]\n",
      "[2024-07-31 10:12:19,456: INFO: helper: 10: ======Helper Functin Started======]\n",
      "[2024-07-31 10:12:20,324: INFO: helper: 38: ======Helper Functin Ended======]\n",
      "[2024-07-31 10:12:20,327: INFO: summary: 57: ======Image Summary Started======]\n",
      "modelling constructor\n",
      "[2024-07-31 10:12:20,330: INFO: modelling: 32: ======Loading Image Model======]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/home/preeti/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-31 10:12:22,469: INFO: modelling: 35: ======Image Model Loaded Successfully======]\n",
      "[2024-07-31 10:12:22,485: INFO: modelling: 32: ======Loading Image Model======]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-31 10:12:24,460: INFO: modelling: 35: ======Image Model Loaded Successfully======]\n",
      "[2024-07-31 10:12:24,472: INFO: summary: 75: ======Table Summary Started======]\n",
      "modelling constructor\n",
      "[2024-07-31 10:12:24,472: INFO: modelling: 14: ======Loading Model======]\n",
      "[2024-07-31 10:12:24,473: INFO: modelling: 20: ======Model Loaded Successfully======]\n",
      "[2024-07-31 10:12:44,854: INFO: helper: 10: ======Helper Functin Started======]\n",
      "[2024-07-31 10:12:45,412: INFO: helper: 38: ======Helper Functin Ended======]\n",
      "[2024-07-31 10:12:45,415: INFO: helper: 10: ======Helper Functin Started======]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/preeti/Desktop/sumit/pdf2ppt/src/helper.py:24: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  sentences = [(sent.text.strip(), sent.similarity(doc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-31 10:12:46,009: INFO: helper: 38: ======Helper Functin Ended======]\n",
      "[2024-07-31 10:12:46,028: INFO: summary: 137: ======Tables Stored in tables Folder======]\n",
      "modelling constructor\n",
      "[2024-07-31 10:12:46,030: INFO: modelling: 24: ======Loading Embedding======]\n",
      "[2024-07-31 10:12:46,030: INFO: modelling: 28: ======Embedding Loaded Successfully======]\n",
      "[2024-07-31 10:12:46,321: INFO: posthog: 20: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.]\n",
      "[2024-07-31 10:12:49,118: INFO: vectordb: 76: ======Embeddings Stored Successfully in Vector-DB======]\n",
      "modelling constructor\n",
      "[2024-07-31 10:12:49,121: INFO: modelling: 14: ======Loading Model======]\n",
      "[2024-07-31 10:12:49,123: INFO: modelling: 20: ======Model Loaded Successfully======]\n",
      "[2024-07-31 10:12:49,234: INFO: pdf_2_ppt: 78: ======Generating PPT Content======]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    You are a power point presentation specialist. You are asked to create\n",
    "    the content for a presentation about {topic}.\n",
    "    You have been given the following information to create a presentation:\n",
    "    ---\n",
    "    {information}.\n",
    "    ---\n",
    "    Structure the information in a way that it can be put in a power point\n",
    "    presentation. Each slide should have a title and content, with the content\n",
    "    being a summary of the information provided. Each slide should have one or\n",
    "    more sentences that capture the key points of the information.\n",
    "    Return the structured information as a JSON as follows.\n",
    "    Your answer should only contain the JSON - no markdown formatting.\n",
    " \"\"\"\n",
    "que = \"attention all you need\"\n",
    "obj = RAG()\n",
    "res,_ = obj.get_response(question=que)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
